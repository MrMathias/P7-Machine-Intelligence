\section{Probabilistic Context-Free Grammars}


\subsection{Context-Free Grammars}
A context-free grammar is a 4-tuple $(N, \Sigma, R, S)$, where
\begin{enumerate}
\item N is a finite set of Non-terminals.   
\item $\Sigma$ is a finite set of terminals 
\item R is a finite set of rules, on the form $X \rightarrow Y_1,Y_2 ... Y_n$,
where $X \in N$ and $Y_i \in N \cup \Sigma$ for $i = 1 ... n$
\item $S \in N$ is the start symbol
\end{enumerate}
\cite[p.104]{sipser}
\cite[p.1]{collins}

\subsubsection{Left-most derivation}
A left-most derivation is a sequence of string $s_1, s_2 ... s_n$ such that
\begin{enumerate}
\item $s_1 = S$
\item $s_n \in \Sigma^*$ ($\Sigma^*$ is the set of all string over the alphabeth $\Sigma$)
\item each $s_i$ is derived from $s_{i-1}$ by picking the left-most Non-terminal in $s_{i-1}$ and replacing it with some $\beta$ where $X \rightarrow \beta$  is a rule in R.
\end{enumerate} 
\cite[p.2]{collins}

\subsubsection{Ambiguity}
A string $w$ is derived ambiguously in a context-free grammar G if it has two or
more different left-most derivations.
A Context-free grammar G is said to be ambiguous if it generates some string ambiguously.
\cite[p.108]{sipser}

When a grammar is ambiguous it can be difficult to determine the meaning of a sentence because
it has multiple parse trees.

\subsection{Probabilistic CFGs}
One way to handle the problem of determining the meaning of ambiguous grammars.
Is by using Probabilistic context-free grammars to 'chose' between the multiple
parse trees that could yield the sentence. 

More formally $\mathcal{T}_G$ is the set of all parse trees under the grammar $G$.
For any derivation $t \in \mathcal{T}_G$ we write $yield(t)$ to denote the string
$s \in \Sigma^*$ that is generated by $t$. 

For a given sentence $s \in \Sigma^*$ we write $\mathcal{T}_G(s)$ to refer to the
set $\{t \in \mathcal{T}_G \vert yield(t) = s \}$.
That is $\mathcal{T}_G(s)$ is the set of all parse trees for the sentence s.

The PCFGs work by extending ordinary CFGs with a probability distribution over 
all possible parse trees of a grammar $G$. Such that for all parse trees 
$$p(t) \geq 0$$ 
and 
$$\sum_{t \in \mathcal{T}_G}p(t) = 1$$ 

Defining the function $p(t)$ could be difficult because of the fact that the set $\mathcal{T}_G$ most likely will be infinite.

\cite[p.6]{collins}

The motivation for defining the function $p(t)$ is that once we have it,
we can determine the probabilities of the different parse trees for a 
string $s$ and get the most likely derivation. 
$$arg \max_{t \in \mathcal{T}_G(s)} p(t)$$

\cite[p.7]{collins}

\subsubsection{Definition of PCFGs}

A PCFG consists of:
\begin{itemize}
\item A CFG $G = (N, \Sigma, S, R)$
\item A parameter $q(\alpha \leftarrow \beta)$
\end{itemize}
For each rule $\alpha \rightarrow \beta \in R$ the parameter $q(\alpha \rightarrow \beta)$ is
the conditional probability that the rule $\alpha \rightarrow \beta$ is chosen in a left-derivation,
given that the non-terminal that is being expanded is $\alpha$.

For any non-terminal $X \in N$ we have the constraint

$$\sum_{\alpha \rightarrow \beta \in R|\alpha=X}q(\alpha \rightarrow \beta) = 1$$

In addition we have the constraint that $q(\alpha \rightarrow \beta) \geq 0$ for any $\alpha \rightarrow \beta \in R$.
Given a parse-tree $t \in \mathcal{T}_G$ containing rules $\alpha_1 \rightarrow \beta_1, \alpha_2 \rightarrow \beta_2, \text{ ... } \alpha_n \rightarrow \beta_n$
The probability of t given the PCFG is

$$p(t) = \prod_{i=1}^n q(\alpha_i \rightarrow \beta_i)$$

\cite[p.7-8]{collins}

Now that we have a way of assigning probabilities to parse-trees it is possible to chose the one with the highest probability and thereby get around the
problem of ambiguity in grammars.

\subsubsection{Deriving a PCFG from a Corpus}
What remains is the question of how to derive a PCFG from a corpus of data.
We assume a training set which consists of parse trees $t_1, t_2, \text{ ... } t_m$

The PCFG (N, $\Sigma$, S, R, q) derived from the corpus is defined as follows
\begin{itemize}
\item N is the set of all non-terminals seen in $t_1, \text{ ... } t_m$
\item $\Sigma$ is the set of all words seen in $t_1, \text{ ... } t_m$
\item S is the start symbol.
\item The set of rules R is taken to be the set of all rules $\alpha \rightarrow \beta$ seen in $t_1, \text{ ... } t_m$
\item The maximum likelyhood parameter estimates are 
$$q_{ML}(\alpha \rightarrow \beta) = \frac{\text{count}(\alpha \rightarrow \beta)}{\text{count}(\alpha)}$$
\end{itemize}
\cite[p.9-10]{collins}

\subsection{Chomsky Normal Form}
A Context-free grammar is in Chomsky normal form if every rule in R is of the form
\begin{enumerate}
\item $A \rightarrow BC$
\item $A \rightarrow \alpha$
\end{enumerate}

where $\alpha$ is any terminal symbol and A,B and C are any Non-terminals except that B and C may not be the start symbol S.
In addition the rule $S \rightarrow \epsilon$ is permitted.
\cite[p.109]{sipser}







\subsubsection{Parsing with PCFG using the CKY Algorithm}

 % write about dynamic programming and how it relates to chomsky normal form.



